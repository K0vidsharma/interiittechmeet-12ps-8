{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7187190,
          "sourceType": "datasetVersion",
          "datasetId": 4155170
        },
        {
          "sourceId": 7187200,
          "sourceType": "datasetVersion",
          "datasetId": 4155177
        },
        {
          "sourceId": 7190050,
          "sourceType": "datasetVersion",
          "datasetId": 4157275
        },
        {
          "sourceId": 7203331,
          "sourceType": "datasetVersion",
          "datasetId": 4167072
        },
        {
          "sourceId": 7203424,
          "sourceType": "datasetVersion",
          "datasetId": 4167145
        }
      ],
      "dockerImageVersionId": 30615,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task-1: Behavior Simulation\n",
        "In this task, we have to predict the user engagement(measured by likes) by using the content of a tweet (text, company, username, media URLs, timestamp)\n",
        "\n",
        "**A Preprocessed Dataset was used for the training.**\n",
        "\n",
        "The original dataset of 300,000 samples was reduced to a representative sample of 50,000. The data was stratified by likes, ensuring an equal representation across different ranges.\n",
        "\n",
        "### Data Preprocessing and feature Engineering steps:\n",
        "* Media type identification was done. Three binary Features \"is_photo\", \"is_gif\" and \"is_video\" were identified from the Media URL.\n",
        "* A time feature(no. of days since tweet) was added to account for the observed temporal trend.\n",
        "* Follower count of each username was introduced using the `follower_count.ipynb` notebook.\n",
        "* Broken Media URLs were managed by replacing missing content with a black screen placeholder for seamless model processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "nrBwg_ZK_ToT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and Importing all the necessary libraries"
      ],
      "metadata": {
        "id": "ZLuTGdv6pdwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install requests\n",
        "!pip install io\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install torchvision\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "zfxhtF1HpdPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F\n",
        "from transformers import ViTModel,ViTImageProcessor\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "trusted": true,
        "id": "t2YGcZtLmhRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the device agnostic code\n",
        "The device agnostic code allows us to use GPU for computation."
      ],
      "metadata": {
        "id": "MhsMOUHw-TCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "trusted": true,
        "id": "xGoz8FmBmhRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class of model architecture\n",
        "\n",
        "### Initializing the model class `__init__()`:\n",
        "\n",
        "1. We get  VisionTransformer and BERTweet model from the HuggingFace library - `transforms`. We use `AutoModel` and `ViTModel` along with the `from_pretrained()` method to get the  weights.\n",
        "2. The model weights are freezed by setting `param.requires_grad` attribute to `False`.\n",
        "3. We create six transformer encoder layers for text and image encoding, two with `nheads=8` (img_enc1 & text_enc1) and rest with `nheads=12`. The activation used is `gelu`. Embedding Dimension(`d_model`) = 768\n",
        "4. Similarly six transformer decoder layers are created for text and image.\n",
        "5. Four fully connected or linear layers are created with `in_features = 768` and `out_features=768`. `self.fc, self.fc1, self.fc2, self.fc3`\n",
        "6. We create four linear layers - `self.corr1`, `self.corr1_1`, `self.corr2` and `self.corr2_1`. Two of them with `in_features = 768` and `out_features = 256`. The other two with `in_features = 256` and `out_features = 64`.\n",
        "7. Similar to the '6', we create four more linear layers. `self.img, self.img_1, self.txt, self.txt_1`\n",
        "\n",
        "8. For the final fully connected layers we create three linear layers:\n",
        "    * `self.FC` with `in_features = 64*4 + 5` and `out_features = 128`. Since we'll get, 4 components with 64 features and 5 extra features.\n",
        "    * `self.FC1` with `in_features = 128` and `out_features = 64`.\n",
        "    * `self.FC2` with `in_features = 64` and `out_features = 1` to get the final output\n",
        "\n",
        "\n",
        "### Creating the `forward()` function:\n",
        "\n",
        "#### Inputs:\n",
        "1. `txt`: content text\n",
        "2. `mask`: attention mask\n",
        "3. `img`: image we get using the url\n",
        "4. `followers`: number of followers\n",
        "5. `time`: time since the tweet\n",
        "6. `is_photo`: Binary feature which tells whether the media is an image.\n",
        "7. `is_gif`: Binary feature which tells whether the media is a gif.\n",
        "8. `is_video`: Binary feature which tells whether the media is a video.\n",
        "\n",
        "#### Returns:\n",
        "The final output or the likes.\n",
        "\n",
        "#### Code Explaination:\n",
        "* We get the `txt_output` and `img_output` from `txt` and `img` respectively, using the base models.\n",
        "* We then pass the text and image outputs, in the first text and image encoder and save them as `[last_hidden_state]` of `txt_output` and `img_output`.\n",
        "* We then perform **cross attention** three times using the image and text encoders and decoders.\n",
        "* We pass the cross attention image and text outputs in two linear layers `self.fc` and `self.fc1` with `gelu` non-linearity and save them in `corr1` and `corr2`.\n",
        "* We then pass the `corr1` and `corr2` in linear layers created in point-6 of the class initialization with `gelu` and `elu` non-linearity to get the out_features = 64.\n",
        "* We get the `pooler_output` from the text and image output of the base models. We pass this input in the linear layers created in point-7 of class initialization with `gelu` and `elu` non-linearity.\n",
        "* We concatenate the cross attention outputs, pooler outputs and five extra features with concatenation dimension `dim = 1`.\n",
        "* We pass the concatenated tensor to the linear layers created in point-8 along with `gelu` and `leaky_relu` non-linearity.\n",
        "\n"
      ],
      "metadata": {
        "id": "_lysfFwAqCH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class transformermodel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(transformermodel, self).__init__()\n",
        "        self.base_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.base_model_1 = AutoModel.from_pretrained('vinai/bertweet-base')\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.base_model_1.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.img_enc1 = nn.TransformerEncoderLayer(768, 8, activation='gelu', batch_first=True)\n",
        "        self.txt_enc1 = nn.TransformerEncoderLayer(768, 8, activation='gelu', batch_first=True)\n",
        "        self.txt_enc2 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "        self.img_enc2 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "        self.txt_enc3 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "        self.img_enc3 = nn.TransformerEncoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "        self.img_dec1 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "        self.txt_dec1 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "        self.txt_dec2 = nn.TransformerDecoderLayer(768, 8, activation='gelu', batch_first=True)\n",
        "        self.img_dec2 = nn.TransformerDecoderLayer(768, 8, activation='gelu', batch_first=True)\n",
        "        self.img_dec3 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "        self.txt_dec3 = nn.TransformerDecoderLayer(768, 12, activation='gelu', batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(768, 768)\n",
        "        self.fc1 = nn.Linear(768, 768)\n",
        "        self.fc2 = nn.Linear(768, 768)\n",
        "        self.fc3 = nn.Linear(768, 768)\n",
        "        self.corr1 = nn.Linear(768, 256)\n",
        "        self.corr1_1 = nn.Linear(256, 64)\n",
        "        self.corr2 = nn.Linear(768, 256)\n",
        "        self.corr2_1 = nn.Linear(256, 64)\n",
        "        self.img = nn.Linear(768, 256)\n",
        "        self.img_1 = nn.Linear(256, 64)\n",
        "        self.txt = nn.Linear(768, 256)\n",
        "        self.txt_1 = nn.Linear(256, 64)\n",
        "        self.FC = nn.Linear(64 * 4 + 5, 128)\n",
        "        self.FC1 = nn.Linear(128, 64)\n",
        "        self.FC2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, txt, mask, img, followers,time, is_photo, is_gif, is_video):\n",
        "        txt_output = self.base_model_1(txt, attention_mask=mask)\n",
        "        img_output = self.base_model(img)\n",
        "        img_output['last_hidden_state'] = (self.img_enc1(img_output['last_hidden_state']))\n",
        "        txt_output['last_hidden_state'] = (self.txt_enc1(txt_output['last_hidden_state']))\n",
        "        cross_attention_img = self.img_dec1(img_output['last_hidden_state'], txt_output['last_hidden_state'])\n",
        "        cross_attention_txt = self.txt_dec1(txt_output['last_hidden_state'], img_output['last_hidden_state'])\n",
        "        cross_attention_img = (self.img_enc2(cross_attention_img))\n",
        "        cross_attention_txt = (self.txt_enc2(cross_attention_txt))\n",
        "        cross_attention_img1 = ((self.img_dec2(cross_attention_img, txt_output['last_hidden_state'])))\n",
        "        cross_attention_txt1 = ((self.txt_dec2(cross_attention_txt, img_output['last_hidden_state'])))\n",
        "        cross_attention_img1 = (self.img_enc3(cross_attention_img1))\n",
        "        cross_attention_txt1 = (self.txt_enc3(cross_attention_txt1))\n",
        "        cross_attention_img2 = (self.img_dec3(cross_attention_img1, txt_output['last_hidden_state']))\n",
        "        cross_attention_txt2 = (self.txt_dec3(cross_attention_txt1, img_output['last_hidden_state']))\n",
        "        corr1 = F.gelu(self.fc(cross_attention_img2[:, 0, :]))\n",
        "        corr2 = F.gelu(self.fc1(cross_attention_txt2[:, 0, :]))\n",
        "        corr1 = F.gelu((self.corr1_1(F.elu((self.corr1(corr1))))))\n",
        "        corr2 = F.gelu((self.corr2_1(F.elu((self.corr2(corr2))))))\n",
        "        img_out = img_output['pooler_output']\n",
        "        txt_out = txt_output['pooler_output']\n",
        "        img_out = F.gelu((self.img_1(F.elu((self.img(img_out))))))\n",
        "        txt_out = F.gelu((self.txt_1(F.elu((self.txt(txt_out))))))\n",
        "        pooler_output = (torch.cat([corr1, corr2, img_out, txt_out, followers, is_photo, is_gif, is_video, time], dim=1))\n",
        "        pooler_output = (F.gelu((self.FC1(F.leaky_relu((self.FC(pooler_output)))))))\n",
        "        return torch.squeeze(self.FC2(pooler_output))"
      ],
      "metadata": {
        "trusted": true,
        "id": "xumxOZ-7mhRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model\n",
        "Replace `model_path.pth` with the actual path for your model file."
      ],
      "metadata": {
        "id": "JyyTxealmhRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model_path.pth', map_location=device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wOABfo8NmhRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to remove emojis or others special characters excluding punctuation marks from the tweet text\n",
        "The Bertweet model tokenizer is not able to tokenize special characters so `preprocess` functions avoids those warnings.\n",
        "\n",
        "**Inputs:**\n",
        "1. `text`: content text\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "Text with no special characters and emojies\n",
        "\n",
        "### Code Expalaination:\n",
        "* The function removes the special characters using the RegEx or Regular Expression Module `import re`.\n",
        "* We use the method `re.compile()` to compile a regular expression pattern of emojies into a regular expression object with `flags = re.UNICODE`.\n",
        "* We then use the method `.sub()` to remove all the emojies.\n",
        "* We remove special characters and replace them with blank string(`\"\"`) by using the method `re.sub()`.\n",
        "* We keep the digits, '#' and basic punctuation marks. We create a list of words by using the method `.split()`.\n",
        "* We create a string with space-seperated words using `.join()` methods.\n",
        "* The fuction then returns the final string."
      ],
      "metadata": {
        "id": "8-p2bzDlmhRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                               u\"\\U000024C2-\"  # Exclude '#' from removal\n",
        "                               u\"\\U0001F251\"  # Exclude '#' from removal\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text_no_emojis = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters, keeping digits, '#' and basic punctuation marks\n",
        "    text_no_special_chars = re.sub(r'[^\\w\\s\\d#.,!?;:()\\'\"]', '', text_no_emojis)\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text_no_special_chars.split()\n",
        "\n",
        "    # Concatenate words into a single string with space-separated words\n",
        "    concatenated_string = ' '.join(words)\n",
        "\n",
        "    return concatenated_string\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "eJodBvCpmhRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Custom Dataset\n",
        "We create a custom dataset which does preprocessing and returns the sample in the form which can be used in the model.\n",
        "* It takes input(`annotations_file`) the path of the excel file.\n",
        "* It inherits `torch.utils.data.Dataset` class\n",
        "### Code Explaination:\n",
        "#### Initializing the class:\n",
        "* we create a DataFrame by reading the excel file using the pandas `.read_excel()` method.\n",
        "* we create a pandas series `dates` as a date time object by using the method `to_datetime()`.\n",
        "* we create a timestamp `curr` by using pandas `.Timestamp()` method.\n",
        "* We create a pattern for the url.\n",
        "* To get the no. of days since tweet by comparing `self.dates` and `curr`.\n",
        "* We create a list of texts `self.texts` from the 'content' column of the dataframe, it contains the text with no emojies and special characters using the function `preprocess()` which we defined in the last code block.\n",
        "* we create list `self.followers` which contains the follower count with type `torch.float`.\n",
        "* we create `self.media` and `self.id`, using the 'media' and 'id' column of the dataframe.\n",
        "* We create three binary features using the `.apply()` function and with `dtype=torch.float`\n",
        "* We create tokenizer `self.txt_tokenizer` for BERTweet model from `AutoTokenizer` and `.from_pretrained()` method.\n",
        "* We create image processor `self.img_processor` for VisionTransformer using `ViTImageProcessor` and `.from_pretrained()` method.\n",
        "* We get the tokenized texts `self.tokenized_texts` from `self.txt_tokenizer` and `self.texts`.\n",
        "\n",
        "#### Creating the `__getitem__(index)` function:\n",
        "For the index = `index` we do the following:\n",
        "* We get the `url` using the RegEx library `re` and `.search()` function.\n",
        "* We use the `.get()` fo `requests` library to get the response.\n",
        "* If the response is an image, we use the `.open()` method of `Image` and convert it to RGB.\n",
        "* We the process the image using  `self.img_processor()` and create a image tensor `img_tensor`.\n",
        "* If we don't get the response as an image, we create a blank image using `torch.zeros(3, 224, 224)`.\n",
        "* We convert the date into a tensor with `dtype = float`.\n",
        "\n",
        "* We get the input ids and attention mask from `self.tokenized_texts`.\n",
        "\n",
        "#### We return all the important information required in form of a dictionary `sample`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DDOc25z6mhRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, annotations_file):\n",
        "        ds = pd.read_excel(annotations_file)\n",
        "        self.dates = pd.to_datetime(ds['date'])\n",
        "        curr = pd.Timestamp('2023-12-05 12:00:00')\n",
        "        self.pattern =  r'https[^\\' ]*\\''\n",
        "        self.dates = ((curr - self.dates).dt.total_seconds() / (24 * 3600))\n",
        "        self.texts = [preprocess(text) for text in ds['content']]\n",
        "        self.followers = torch.tensor(ds['followers'], dtype=torch.float)\n",
        "        self.media = ds['media']\n",
        "        self.id = ds['id']\n",
        "        self.is_photo = torch.tensor(ds['media'].apply(lambda x: 1 if len(x) >= 2 and x[1] == 'P' else 0), dtype=torch.float)\n",
        "        self.is_video = torch.tensor(ds['media'].apply(lambda x: 1 if len(x) >= 2 and x[1] == 'V' else 0), dtype=torch.float)\n",
        "        self.is_gif = torch.tensor(ds['media'].apply(lambda x: 1 if len(x) >= 2 and x[1] == 'G' else 0), dtype=torch.float)\n",
        "        self.img_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "        self.txt_tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
        "\n",
        "        # Tokenize all texts during initialization\n",
        "        self.tokenized_texts = self.txt_tokenizer(self.texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dates)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        followers = self.followers[index]\n",
        "        is_photo = self.is_photo[index]\n",
        "        is_video = self.is_video[index]\n",
        "        is_gif = self.is_gif[index]\n",
        "        url = re.search(self.pattern, self.media[index])\n",
        "        try:\n",
        "            response = requests.get(url[0][0:-1])\n",
        "            if response.status_code == 200:\n",
        "                image = Image.open(BytesIO(response.content))\n",
        "                if image.mode == 'RGBA':\n",
        "                    image = image.convert('RGB')\n",
        "                img_tensor = torch.tensor(self.img_processor(image)['pixel_values'][0], dtype=torch.float)\n",
        "            else:\n",
        "                img_tensor = torch.zeros(3, 224, 224)\n",
        "        except Exception as e:\n",
        "            img_tensor = torch.zeros(3, 224, 224)\n",
        "        time = torch.tensor(self.dates[index], dtype=torch.float)\n",
        "\n",
        "        # Access input_ids and attention_mask from tokenized_texts\n",
        "        input_ids = self.tokenized_texts['input_ids'][index]\n",
        "        attention_mask = self.tokenized_texts['attention_mask'][index]\n",
        "\n",
        "        sample = {\n",
        "            \"Text\": {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "            },\n",
        "            \"followers\": torch.unsqueeze(followers,0),\n",
        "            \"img\": img_tensor,\n",
        "            \"time\": torch.unsqueeze(time,0),\n",
        "            \"is_photo\" : torch.unsqueeze(is_photo,0),\n",
        "            \"is_video\" : torch.unsqueeze(is_video,0),\n",
        "            \"is_gif\" : torch.unsqueeze(is_gif,0),\n",
        "            \"id\" : self.id[index],\n",
        "        }\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "trusted": true,
        "id": "b7Yjv-8MmhRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an instance of the dataset and dataloader\n",
        "Replace '/kaggle/input/dataset/PS_8_dataset_with_followers.xlsx' with the actual path for your dataset excel file."
      ],
      "metadata": {
        "id": "unHbIYoUmhRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = CustomDataset('/kaggle/input/test-time/behaviuor_simulation_test_time_followers.xlsx')  # this is a torch dataset instance\n",
        "df = pd.read_excel('/kaggle/input/test-time/behaviuor_simulation_test_time_followers.xlsx')  # this is a pandas dataframe"
      ],
      "metadata": {
        "trusted": true,
        "id": "T1-6IsBamhRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the dataloader using the `DataLoader` function. For testing, we set the `batch_size = 1`, `shuffle=False` and `num_workers = os.cpu_count()`"
      ],
      "metadata": {
        "id": "cOuGHheAd4L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the dataloader\n",
        "dataloader = DataLoader(ds, batch_size=1, shuffle=False, num_workers=os.cpu_count())"
      ],
      "metadata": {
        "trusted": true,
        "id": "B59eMmN1mhRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting likes for the tweets\n",
        "\n",
        "* We set the model in evaluation mode and use `torch.no_grad()` to stop tracking.\n",
        "* We get all the variables from `data` and save them to `device`.\n",
        "* We get the outputs from the model and input variables.\n"
      ],
      "metadata": {
        "id": "FegvPEVAmhRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []  # all the outputs will be stored in this list\n",
        "model.eval()\n",
        "c = 0\n",
        "with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "        c+=1\n",
        "        try:\n",
        "            text,mask,imgs,followers,time,is_photo,is_gif,is_video = data['Text']['input_ids'],data['Text']['attention_mask'],data['img'],data['followers'],data['time'],data['is_photo'],data['is_gif'],data['is_video']\n",
        "            text,mask,imgs,followers,time,is_photo,is_gif,is_video = text.to(device),mask.to(device),imgs.to(device),followers.to(device),time.to(device),is_photo.to(device),is_gif.to(device),is_video.to(device)\n",
        "\n",
        "            output = model(text,mask,imgs,followers,time,is_photo,is_gif,is_video)\n",
        "            outputs.append(output.item())\n",
        "        except Exception as e:\n",
        "            print(data['id'])\n",
        "            print(e)\n",
        "            break\n",
        "        if c%500 == 499:\n",
        "            print(c)"
      ],
      "metadata": {
        "trusted": true,
        "id": "I0xD8bt7mhRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(len(outputs)):\n",
        "    outputs[idx] = int(outputs[idx])"
      ],
      "metadata": {
        "trusted": true,
        "id": "CtWeI78SmhRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a new column 'predicted_likes' in the dataframe"
      ],
      "metadata": {
        "id": "k8gpMZDTmhRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['predicted_likes'] = outputs"
      ],
      "metadata": {
        "trusted": true,
        "id": "Oq_E3GbdmhRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the dataframe in the form of excel file"
      ],
      "metadata": {
        "id": "-ZqOL_fYmhRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('behaviour_simulation_test_output.xlsx')"
      ],
      "metadata": {
        "trusted": true,
        "id": "8n-bqlwXmhRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}